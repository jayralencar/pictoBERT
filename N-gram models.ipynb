{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github//jayralencar/pictoBERT/blob/main/N-gram models.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/jayralencar/pictoBERT/blob/main/N-gram models.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "H_toUpMuXuYu"
      },
      "id": "H_toUpMuXuYu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram models\n",
        "\n",
        "This notebook presents the procedure for creating the n-gram models to compare with PictoBERT."
      ],
      "metadata": {
        "id": "rg-VgJ2AX1E1"
      },
      "id": "rg-VgJ2AX1E1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a014a69",
      "metadata": {
        "id": "2a014a69"
      },
      "outputs": [],
      "source": [
        "!wget http://jayr.clubedosgeeks.com.br/pictobert/train_childes_all.pt\n",
        "!wget http://jayr.clubedosgeeks.com.br/pictobert/test_childes_all.pt\n",
        "!wget http://jayr.clubedosgeeks.com.br/pictobert/val_childes_all.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3617d8be",
      "metadata": {
        "id": "3617d8be",
        "outputId": "0d176e75-b482-4410-87ba-f1c57fbf7daf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/capsule/files\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-2g-GCxjBwESqDn3JByAJABU9Dkuqy0m\n",
            "To: /root/capsule/files/childes_all_new.json\n",
            "100%|████████████████████████████████████████| 332k/332k [00:00<00:00, 8.90MB/s]\n",
            "/root/capsule\n"
          ]
        }
      ],
      "source": [
        "!wget http://jayr.clubedosgeeks.com.br/pictobert/childes_all_new.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4269da",
      "metadata": {
        "id": "4a4269da"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATASET_PATH = \"./train_childes_all.pt\"\n",
        "VAL_DATASET_PATH = \"./val_childes_all.pt\"\n",
        "TEST_DATASET_PATH = \"./test_childes_all.pt\"\n",
        "\n",
        "TOKENIZER_PATH = \"./childes_all_new.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b78fed",
      "metadata": {
        "id": "95b78fed",
        "outputId": "e6531101-3512-4d05-fa7b-38a1c6a8af44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 29.4MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.2) (8.0.4)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.2) (4.32.1)\n",
            "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.2) (2022.3.15)\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.2) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from click->nltk==3.6.2) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk==3.6.2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk==3.6.2) (3.7.0)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed nltk-3.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk==3.6.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e1a716",
      "metadata": {
        "id": "79e1a716"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "loaded_tokenizer = PreTrainedTokenizerFast(tokenizer_file=TOKENIZER_PATH)\n",
        "loaded_tokenizer.pad_token = \"[PAD]\"\n",
        "loaded_tokenizer.sep_token = \"[SEP]\"\n",
        "loaded_tokenizer.mask_token = \"[MASK]\"\n",
        "loaded_tokenizer.cls_token = \"[CLS]\"\n",
        "loaded_tokenizer.unk_token = \"[UNK]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd7ce2e",
      "metadata": {
        "id": "afd7ce2e"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "train_dataset = pickle.load(open(TRAIN_DATASET_PATH,'rb'))\n",
        "val_dataset = pickle.load(open(VAL_DATASET_PATH,'rb'))\n",
        "test_dataset = pickle.load(open(TEST_DATASET_PATH,'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf6257c",
      "metadata": {
        "id": "1cf6257c"
      },
      "outputs": [],
      "source": [
        "train = [loaded_tokenizer.convert_ids_to_tokens(s,skip_special_tokens=True) for s in train_dataset['input_ids'] + val_dataset['input_ids']]\n",
        "test = [loaded_tokenizer.convert_ids_to_tokens(s,skip_special_tokens=True) for s in test_dataset['input_ids']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312e736a",
      "metadata": {
        "id": "312e736a"
      },
      "outputs": [],
      "source": [
        "from nltk.lm import Vocabulary\n",
        "from nltk.lm.preprocessing import flatten\n",
        "all = list(flatten(train+test))\n",
        "vocab = Vocabulary(all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "860792d6",
      "metadata": {
        "id": "860792d6"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm import Laplace\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a5f6d7",
      "metadata": {
        "id": "94a5f6d7",
        "outputId": "6041753c-f8e9-4429-90f3-c748d6d39fb0"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (95185168.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_166/95185168.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    cd ..\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "%mkdir ngram-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2a3c47f",
      "metadata": {
        "id": "f2a3c47f"
      },
      "outputs": [],
      "source": [
        "max_order = 10\n",
        "entropies = []\n",
        "ppls = []\n",
        "for order in range(2,11):\n",
        "  train_data = [list(ngrams(sentence,order,pad_left=True,left_pad_symbol=\"[PAD]\")) for sentence in train]\n",
        "  test_data = [list(ngrams(sentence,order,pad_left=True,left_pad_symbol=\"[PAD]\")) for sentence in test]\n",
        "\n",
        "  lm = MLE(order)\n",
        "\n",
        "  # lm.fit(train_data, vocabulary_text=list(set(all)))\n",
        "\n",
        "  lm.fit(train_data, vocabulary_text=list(set(all)))\n",
        "  lm.fit(train_data)\n",
        "\n",
        "  with open(\"./ngram-models/{0}-gram_model.pk\".format(order),'wb') as fout:\n",
        "    pickle.dump(lm, fout)\n",
        "\n",
        "  # all_scores = [lm.logscore(a[-1],a[:-1]) if lm.logscore(a[-1],a[:-1]) != -math.inf else 0.0 for a in flatten(test_data)]\n",
        "  # entropy = -1 * np.mean(all_scores)\n",
        "  list_ = [lm.entropy(test) for test in test_data if lm.entropy(test) != math.inf]\n",
        "  entropy = np.array(list_).mean()\n",
        "  entropies.append(entropy)\n",
        "  ppls.append(math.exp(entropy))\n",
        "  print(\"Order:\", order)\n",
        "  print(\"Cross Entropy:\", entropy)\n",
        "  print(\"Perplexity:\", math.exp(entropy))\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "N-gram models.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}