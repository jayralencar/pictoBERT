{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a014a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://jayr.clubedosgeeks.com.br/pictobert/train_childes_all.pt\n",
    "!wget http://jayr.clubedosgeeks.com.br/pictobert/test_childes_all.pt\n",
    "!wget http://jayr.clubedosgeeks.com.br/pictobert/val_childes_all.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3617d8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/capsule/files\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-2g-GCxjBwESqDn3JByAJABU9Dkuqy0m\n",
      "To: /root/capsule/files/childes_all_new.json\n",
      "100%|████████████████████████████████████████| 332k/332k [00:00<00:00, 8.90MB/s]\n",
      "/root/capsule\n"
     ]
    }
   ],
   "source": [
    "!wget http://jayr.clubedosgeeks.com.br/pictobert/childes_all_new.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4269da",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = \"./train_childes_all.pt\"\n",
    "VAL_DATASET_PATH = \"./val_childes_all.pt\"\n",
    "TEST_DATASET_PATH = \"./test_childes_all.pt\"\n",
    "\n",
    "TOKENIZER_PATH = \"./childes_all_new.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b78fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.6.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 29.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.2) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.2) (4.32.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.2) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.2) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from click->nltk==3.6.2) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk==3.6.2) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk==3.6.2) (3.7.0)\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e1a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "loaded_tokenizer = PreTrainedTokenizerFast(tokenizer_file=TOKENIZER_PATH)\n",
    "loaded_tokenizer.pad_token = \"[PAD]\"\n",
    "loaded_tokenizer.sep_token = \"[SEP]\"\n",
    "loaded_tokenizer.mask_token = \"[MASK]\"\n",
    "loaded_tokenizer.cls_token = \"[CLS]\"\n",
    "loaded_tokenizer.unk_token = \"[UNK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd7ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_dataset = pickle.load(open(TRAIN_DATASET_PATH,'rb'))\n",
    "val_dataset = pickle.load(open(VAL_DATASET_PATH,'rb'))\n",
    "test_dataset = pickle.load(open(TEST_DATASET_PATH,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf6257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [loaded_tokenizer.convert_ids_to_tokens(s,skip_special_tokens=True) for s in train_dataset['input_ids'] + val_dataset['input_ids']]\n",
    "test = [loaded_tokenizer.convert_ids_to_tokens(s,skip_special_tokens=True) for s in test_dataset['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "312e736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.preprocessing import flatten\n",
    "all = list(flatten(train+test))\n",
    "vocab = Vocabulary(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "860792d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Laplace\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a5f6d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (95185168.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_166/95185168.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    cd ..\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%mkdir ngram-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_order = 10\n",
    "entropies = []\n",
    "ppls = []\n",
    "for order in range(2,11):\n",
    "  train_data = [list(ngrams(sentence,order,pad_left=True,left_pad_symbol=\"[PAD]\")) for sentence in train]\n",
    "  test_data = [list(ngrams(sentence,order,pad_left=True,left_pad_symbol=\"[PAD]\")) for sentence in test]\n",
    "\n",
    "  lm = MLE(order)\n",
    "\n",
    "  # lm.fit(train_data, vocabulary_text=list(set(all)))\n",
    "\n",
    "  lm.fit(train_data, vocabulary_text=list(set(all)))\n",
    "  lm.fit(train_data)\n",
    "\n",
    "  with open(\"./ngram-models/{0}-gram_model.pk\".format(order),'wb') as fout:\n",
    "    pickle.dump(lm, fout)\n",
    "\n",
    "  # all_scores = [lm.logscore(a[-1],a[:-1]) if lm.logscore(a[-1],a[:-1]) != -math.inf else 0.0 for a in flatten(test_data)]\n",
    "  # entropy = -1 * np.mean(all_scores)\n",
    "  list_ = [lm.entropy(test) for test in test_data if lm.entropy(test) != math.inf]\n",
    "  entropy = np.array(list_).mean()\n",
    "  entropies.append(entropy)\n",
    "  ppls.append(math.exp(entropy))\n",
    "  print(\"Order:\", order)\n",
    "  print(\"Cross Entropy:\", entropy)\n",
    "  print(\"Perplexity:\", math.exp(entropy))\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
